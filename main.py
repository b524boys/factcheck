#!/usr/bin/env python
import os
import sys
import time
import json
import argparse
import requests
from urllib.parse import urljoin
import logging
from langchain_google_community import GoogleSearchAPIWrapper
from langchain_core.tools import Tool
from tqdm import tqdm
import concurrent.futures
import traceback
import http.client
import ssl
import random
import signal
import atexit
import tempfile
from pathlib import Path

from search_query_filter import filter_search_queries

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("client.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("fact_check_client")


def get_temp_dir():
    """è·å–é€‚åˆå½“å‰ç³»ç»Ÿçš„ä¸´æ—¶ç›®å½•"""
    if sys.platform.startswith('win'):
        temp_dirs = [
            'C:/Users/Cleme/Code/Work/factcheck/temp',
            os.path.expanduser('~/temp'),
            tempfile.gettempdir()
        ]
        
        for temp_dir in temp_dirs:
            try:
                Path(temp_dir).mkdir(parents=True, exist_ok=True)
                return temp_dir
            except:
                continue
        
        local_temp = os.path.join(os.getcwd(), 'temp')
        Path(local_temp).mkdir(parents=True, exist_ok=True)
        return local_temp
    else:
        return '/tmp'


def safe_print(message, use_emoji=True):
    try:
        if not use_emoji:
            message = message.replace('âœ…', '[SUCCESS]')
            message = message.replace('âŒ', '[ERROR]')
            message = message.replace('ğŸš€', '[START]')
            message = message.replace('ğŸ“Š', '[RESULT]')
            message = message.replace('âš ï¸', '[WARNING]')
        print(message)
    except UnicodeEncodeError:
        safe_msg = ''.join(char if ord(char) < 128 else '?' for char in message)
        print(safe_msg)


class CircuitBreaker:

    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = "CLOSED"

    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.failure_count = 0
        self.state = "CLOSED"

    def record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"

    def can_execute(self):
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œè¯·æ±‚

        Returns:
            bool: å¦‚æœæ–­è·¯å™¨å…è®¸æ‰§è¡Œè¯·æ±‚åˆ™ä¸ºTrue, å¦åˆ™ä¸ºFalse
        """
        now = time.time()

        if self.state == "OPEN":
            if now - self.last_failure_time > self.recovery_timeout:
                self.state = "HALF-OPEN"
                return True
            return False

        return True

    def __str__(self):
        return f"CircuitBreaker(state={self.state}, failures={self.failure_count})"


class FactCheckClient:
    def __init__(self, server_url, api_key=None, search_engine_id=None, max_retries=3, max_workers=4):
        """
        åˆå§‹åŒ–äº‹å®æ ¸æŸ¥å®¢æˆ·ç«¯

        Args:
            server_url: æœåŠ¡å™¨URL
            api_key: Google APIå¯†é’¥
            search_engine_id: Googleæœç´¢å¼•æ“ID
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            max_workers: å¹¶è¡ŒæŸ¥è¯¢å·¥ä½œçº¿ç¨‹æ•°
        """
        self.server_url = server_url.rstrip('/')
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        self.search_engine_id = search_engine_id or os.environ.get("GOOGLE_CSE_ID")
        self.max_retries = max_retries
        self.max_workers = max_workers

        self.claim_cache = {}

        self.current_task_id = None

        self.search_circuit_breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=120)

        self.session = self._create_robust_session()

        if not self.api_key or not self.search_engine_id:
            logger.warning("æœªè®¾ç½®Google APIå¯†é’¥æˆ–æœç´¢å¼•æ“IDï¼å°†æ— æ³•æ‰§è¡Œæœç´¢æŸ¥è¯¢")
            self.search = None
            self.search_tool = None
        else:
            os.environ["GOOGLE_API_KEY"] = self.api_key
            os.environ["GOOGLE_CSE_ID"] = self.search_engine_id

            try:
                logger.info(f"åˆå§‹åŒ–GoogleSearchAPIWrapperï¼Œè¿”å›å‰5æ¡ç»“æœ...")
                self.search = GoogleSearchAPIWrapper(k=5)

                self.search_tool = Tool(
                    name="Google Search",
                    description="Search Google for recent results.",
                    func=self.search.run
                )
                logger.info("å·²æˆåŠŸåˆå§‹åŒ–Googleæœç´¢å·¥å…·")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–æœç´¢å·¥å…·æ—¶å‡ºé”™: {str(e)}")
                logger.debug(traceback.format_exc())
                self.search = None
                self.search_tool = None

        self._register_cleanup_handlers()

    def _register_cleanup_handlers(self):
        """æ³¨å†Œæ¸…ç†å¤„ç†å‡½æ•°"""

        def cleanup_handler(signum, frame):
            logger.info("æ¥æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
            self._cleanup_on_exit()
            sys.exit(0)

        def cleanup_at_exit():
            logger.info("ç¨‹åºæ­£å¸¸é€€å‡ºï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
            self._cleanup_on_exit()

        signal.signal(signal.SIGINT, cleanup_handler)
        signal.signal(signal.SIGTERM, cleanup_handler)

        atexit.register(cleanup_at_exit)

    def _cleanup_on_exit(self):
        """å®¢æˆ·ç«¯é€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
        if self.current_task_id:
            try:
                logger.info(f"é€šçŸ¥æœåŠ¡å™¨å®¢æˆ·ç«¯é€€å‡ºï¼Œä»»åŠ¡ID: {self.current_task_id}")
                self._notify_server_client_exit(self.current_task_id)
            except Exception as e:
                logger.error(f"é€šçŸ¥æœåŠ¡å™¨å¤±è´¥: {e}")

    def _notify_server_client_exit(self, task_id):
        """é€šçŸ¥æœåŠ¡å™¨å®¢æˆ·ç«¯é€€å‡º"""
        try:
            url = urljoin(self.server_url, f"client_exit/{task_id}")
            response = self._make_request('post', url, timeout=10)
            if response and response.status_code == 200:
                logger.info("æˆåŠŸé€šçŸ¥æœåŠ¡å™¨å®¢æˆ·ç«¯é€€å‡º")
            else:
                logger.warning("é€šçŸ¥æœåŠ¡å™¨å¤±è´¥æˆ–æ— å“åº”")
        except Exception as e:
            logger.error(f"é€šçŸ¥æœåŠ¡å™¨æ—¶å‡ºé”™: {e}")

    def _create_robust_session(self):
        """åˆ›å»ºå…·æœ‰é‡è¯•èƒ½åŠ›çš„è¯·æ±‚ä¼šè¯"""
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )

        session = requests.Session()
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        return session

    def _make_request(self, method, url, **kwargs):
        """
        å‘é€è¯·æ±‚ï¼Œå¹¶å¤„ç†å¼‚å¸¸å’Œé‡è¯•

        Args:
            method: è¯·æ±‚æ–¹æ³• ('get' æˆ– 'post')
            url: è¯·æ±‚URL
            **kwargs: è¯·æ±‚å‚æ•°

        Returns:
            å“åº”å¯¹è±¡æˆ–None(å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥)
        """
        kwargs.setdefault('timeout', 120)
        retries = kwargs.pop('retries', self.max_retries)

        request_func = getattr(self.session, method.lower())

        for attempt in range(retries + 1):
            try:
                if attempt > 0:
                    delay = 2 ** (attempt - 1)
                    logger.info(f"ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)

                logger.debug(f"å‘é€{method.upper()}è¯·æ±‚: {url}")
                response = request_func(url, **kwargs)
                return response

            except requests.exceptions.Timeout as e:
                logger.warning(f"è¯·æ±‚è¶…æ—¶: {e}")
                if attempt == retries:
                    logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚")
                    return None

            except requests.exceptions.ConnectionError as e:
                logger.warning(f"è¿æ¥é”™è¯¯: {e}")
                if "RemoteDisconnected" in str(e) or "ConnectionResetError" in str(e):
                    logger.warning("æœåŠ¡å™¨æ–­å¼€è¿æ¥ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨å¿™æˆ–è¯·æ±‚å¤ªå¤§")
                    kwargs['timeout'] = kwargs.get('timeout', 120) + 60

                if attempt == retries:
                    logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚")
                    return None

            except Exception as e:
                logger.error(f"è¯·æ±‚å‡ºé”™: {e}")
                logger.debug(traceback.format_exc())
                if attempt == retries:
                    logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚")
                    return None

        return None

    def check_server(self):
        """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿"""
        try:
            response = self._make_request('get', urljoin(self.server_url, "health"), timeout=5)
            if response and response.status_code == 200:
                logger.info("æœåŠ¡å™¨åœ¨çº¿ä¸”è¿è¡Œæ­£å¸¸")
                return True
            else:
                status_code = response.status_code if response else "æœªçŸ¥"
                logger.error(f"æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç : {status_code}")
                return False
        except Exception as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {e}")
            return False

    def submit_task(self, claim, media_path=None):
        """
        æäº¤äº‹å®æ ¸æŸ¥ä»»åŠ¡ï¼Œå¹¶ç¼“å­˜å£°æ˜

        Args:
            claim: éœ€è¦æ ¸æŸ¥çš„å£°æ˜
            media_path: åª’ä½“æ–‡ä»¶è·¯å¾„(å¯é€‰)

        Returns:
            task_id æˆ– None(å¦‚æœå‡ºé”™)
        """
        try:
            data = {'claim': claim}
            files = {}

            if media_path and os.path.exists(media_path):
                file_size = os.path.getsize(media_path) / (1024 * 1024)
                logger.info(f"æ­£åœ¨å‡†å¤‡ä¸Šä¼ åª’ä½“æ–‡ä»¶: {media_path} (å¤§å°: {file_size:.2f} MB)")

                timeout = max(120, int(file_size * 3))
                logger.info(f"è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º {timeout} ç§’")

                files['media'] = (os.path.basename(media_path), open(media_path, 'rb'))
            elif media_path:
                logger.error(f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨: {media_path}")
                return None

            url = urljoin(self.server_url, "submit_task")

            logger.info("å¼€å§‹ä¸Šä¼ æ–‡ä»¶å’Œæäº¤ä»»åŠ¡...")
            response = self._make_request('post', url, data=data, files=files,
                                          timeout=timeout if 'timeout' in locals() else 180,
                                          stream=True)

            if media_path and os.path.exists(media_path) and 'media' in files:
                files['media'][1].close()

            if response and response.status_code == 200:
                result = response.json()
                task_id = result.get('task_id')
                logger.info(f"ä»»åŠ¡æäº¤æˆåŠŸï¼Œä»»åŠ¡ID: {task_id}")

                self.claim_cache[task_id] = claim
                logger.info(f"å·²ç¼“å­˜å£°æ˜: '{claim}' å¯¹åº”ä»»åŠ¡ID: {task_id}")

                self.current_task_id = task_id

                return task_id
            else:
                status_code = response.status_code if response else "è¿æ¥å¤±è´¥"
                error_text = response.text if response else "æ— å“åº”"
                logger.error(f"ä»»åŠ¡æäº¤å¤±è´¥: {status_code} - {error_text}")

                if response and response.status_code == 413:
                    logger.error("æ–‡ä»¶å¯èƒ½å¤ªå¤§ï¼Œè¶…å‡ºæœåŠ¡å™¨é™åˆ¶")
                elif not response:
                    logger.error("æœåŠ¡å™¨æ²¡æœ‰å“åº”ï¼Œå¯èƒ½æ˜¯å¤„ç†åª’ä½“æ–‡ä»¶æ—¶å´©æºƒæˆ–è¶…æ—¶")

                return None

        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            logger.debug(traceback.format_exc())
            return None

    def perform_search(self, query, retry=0):
        """
        æ‰§è¡ŒGoogleæœç´¢æŸ¥è¯¢ï¼Œå¢å¼ºç‰ˆæœ¬

        Args:
            query: æœç´¢æŸ¥è¯¢
            retry: å½“å‰é‡è¯•æ¬¡æ•°

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.search_tool:
            logger.error("æœç´¢å·¥å…·æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
            return []

        try:
            logger.info(f"æ­£åœ¨æœç´¢: '{query}'")
            start_time = time.time()

            result = self.search_tool.run(query)

            elapsed_time = time.time() - start_time
            logger.info(f"æœç´¢å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

            if result:
                paragraphs = [p.strip() for p in result.split("\n\n") if p.strip()]

                if len(paragraphs) <= 1 and len(result) > 100:
                    paragraphs = [s.strip() + "." for s in result.split(". ") if s.strip()]

                if not paragraphs:
                    paragraphs = [result]

                logger.info(f"æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                return paragraphs
            else:
                logger.warning(f"æŸ¥è¯¢ '{query}' æ²¡æœ‰è¿”å›ç»“æœ")
                return []

        except (requests.exceptions.ConnectionError,
                requests.exceptions.ChunkedEncodingError,
                requests.exceptions.ReadTimeout,
                requests.exceptions.SSLError,
                http.client.RemoteDisconnected,
                http.client.IncompleteRead,
                ConnectionResetError,
                ssl.SSLError) as e:
            error_type = type(e).__name__
            logger.error(f"æœç´¢æŸ¥è¯¢ '{query}' å‘ç”Ÿç½‘ç»œé”™è¯¯: {error_type} - {str(e)}")

            if retry < self.max_retries:
                base_delay = min(30, 2 ** retry)
                jitter = random.uniform(0, 1)
                delay = base_delay + jitter

                logger.info(f"ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {delay:.2f} ç§’åé‡è¯• ({retry + 1}/{self.max_retries})...")
                time.sleep(delay)

                if isinstance(e, ssl.SSLError) or "SSL" in str(e):
                    logger.info("æ£€æµ‹åˆ°SSLé”™è¯¯ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–æœç´¢å·¥å…·...")
                    try:
                        self.search = GoogleSearchAPIWrapper(k=5)
                        self.search_tool = Tool(
                            name="Google Search",
                            description="Search Google for recent results.",
                            func=self.search.run
                        )
                    except Exception as init_error:
                        logger.error(f"é‡æ–°åˆå§‹åŒ–æœç´¢å·¥å…·å¤±è´¥: {str(init_error)}")

                return self.perform_search(query, retry + 1)

            logger.error(f"æœç´¢æŸ¥è¯¢ '{query}' è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retries}ï¼Œè¿”å›ç©ºç»“æœ")
            return []

        except Exception as e:
            logger.error(f"æœç´¢æŸ¥è¯¢ '{query}' å‡ºé”™: {str(e)}")
            logger.debug(traceback.format_exc())

            if retry < self.max_retries:
                delay = 2 ** retry + random.uniform(0, 1)
                logger.info(f"æœç´¢å¤±è´¥ï¼Œç­‰å¾… {delay:.2f} ç§’åé‡è¯• ({retry + 1}/{self.max_retries})...")
                time.sleep(delay)
                return self.perform_search(query, retry + 1)

            return []

    def process_queries(self, task_id, max_wait_time=300):
        """
        ä»æœåŠ¡å™¨è·å–æŸ¥è¯¢è¯·æ±‚å¹¶æ‰§è¡Œæœç´¢

        Args:
            task_id: ä»»åŠ¡ID
            max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´(ç§’)ï¼Œé»˜è®¤5åˆ†é’Ÿ

        Returns:
            TrueæˆåŠŸï¼ŒFalseå¤±è´¥
        """
        try:
            start_time = time.time()
            claim = None

            if task_id in self.claim_cache:
                claim = self.claim_cache[task_id]
                logger.info(f"ä»ç¼“å­˜ä¸­è·å–åˆ°å£°æ˜: '{claim}'")

            logger.info(f"ç­‰å¾…æœåŠ¡å™¨å¤„ç†ä»»åŠ¡ï¼Œæœ€é•¿ç­‰å¾… {max_wait_time} ç§’...")

            while time.time() - start_time < max_wait_time:
                url = urljoin(self.server_url, f"get_queries/{task_id}")
                response = self._make_request('get', url, timeout=30)

                if not response:
                    logger.error("è·å–æŸ¥è¯¢è¯·æ±‚å¤±è´¥ï¼ŒæœåŠ¡å™¨æ— å“åº”")
                    return False

                if response.status_code != 200:
                    logger.error(f"è·å–æŸ¥è¯¢å¤±è´¥: {response.status_code} - {response.text}")
                    return False

                result = response.json()

                if not claim and "claim" in result:
                    claim = result.get("claim")
                    logger.info(f"ä»æœåŠ¡å™¨å“åº”è·å–åˆ°å£°æ˜: '{claim}'")
                    self.claim_cache[task_id] = claim

                if result.get("status") == "error":
                    logger.error(f"ä»»åŠ¡å‡ºé”™: {result.get('error')}")
                    logger.debug(f"é”™è¯¯è¯¦æƒ…: {result.get('traceback')}")
                    return False

                if result.get("status") == "processing":
                    elapsed = int(time.time() - start_time)
                    remaining = max_wait_time - elapsed
                    logger.info(f"ä»»åŠ¡ä»åœ¨å¤„ç†ä¸­ï¼Œå·²ç­‰å¾… {elapsed}sï¼Œå‰©ä½™ {remaining}sï¼Œç­‰å¾…10ç§’åé‡è¯•...")
                    time.sleep(10)
                    continue

                if result.get("status") == "success":
                    queries = result.get("queries", [])
                    queries = filter_search_queries(queries)

                    if claim and claim not in queries:
                        queries.append(claim)
                        logger.info(f"å°†å£°æ˜æ·»åŠ åˆ°æŸ¥è¯¢åˆ—è¡¨: '{claim}'")

                    total_queries = len(queries)

                    if not queries:
                        logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„æŸ¥è¯¢")
                        return True

                    logger.info(f"æ”¶åˆ° {total_queries} ä¸ªéœ€è¦å¤„ç†çš„æŸ¥è¯¢")
                    break

                elapsed = int(time.time() - start_time)
                remaining = max_wait_time - elapsed
                logger.warning(
                    f"ä»»åŠ¡çŠ¶æ€ä¸º '{result.get('status')}', å·²ç­‰å¾… {elapsed}sï¼Œå‰©ä½™ {remaining}sï¼Œç­‰å¾…10ç§’åé‡è¯•...")
                time.sleep(10)

            if not 'queries' in locals() or not queries:
                logger.error(f"ç­‰å¾…è¶…æ—¶({max_wait_time}ç§’)ï¼Œæ— æ³•è·å–æŸ¥è¯¢")
                return False

            query_results = {}

            if not self.search_circuit_breaker.can_execute():
                logger.warning("æœç´¢æ–­è·¯å™¨å·²æ‰“å¼€ï¼Œè·³è¿‡æ‰€æœ‰æœç´¢æŸ¥è¯¢ã€‚ç³»ç»Ÿå°†åœ¨ç¨åå°è¯•æ¢å¤ã€‚")
                for query in queries:
                    query_results[query] = []
            else:
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_query = {
                        executor.submit(self.perform_search, query): query
                        for query in queries
                    }

                    search_failures = 0

                    with tqdm(total=len(future_to_query), desc="æ‰§è¡Œæœç´¢æŸ¥è¯¢", unit="query") as pbar:
                        for future in concurrent.futures.as_completed(future_to_query):
                            query = future_to_query[future]
                            try:
                                results = future.result()
                                query_results[query] = results

                                if results:
                                    self.search_circuit_breaker.record_success()
                                    logger.info(f"æŸ¥è¯¢ '{query}' æˆåŠŸè·å– {len(results)} æ¡ç»“æœ")
                                else:
                                    logger.warning(f"æŸ¥è¯¢ '{query}' æœªè¿”å›ç»“æœ")
                            except Exception as e:
                                logger.error(f"å¤„ç†æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
                                query_results[query] = []

                                self.search_circuit_breaker.record_failure()
                                search_failures += 1
                            pbar.update(1)

                    if search_failures > len(queries) // 2:
                        logger.warning(
                            f"è¶…è¿‡ä¸€åŠçš„æŸ¥è¯¢å¤±è´¥ ({search_failures}/{len(queries)})ï¼Œæ–­è·¯å™¨çŠ¶æ€: {self.search_circuit_breaker}")

            logger.info(f"æ­£åœ¨æäº¤ {len(query_results)} ä¸ªæŸ¥è¯¢çš„ç»“æœ")

            success = self.submit_query_results_with_confirmation(task_id, query_results)

            if success:
                logger.info("æ‰€æœ‰æŸ¥è¯¢ç»“æœå·²æˆåŠŸæäº¤å¹¶ç¡®è®¤")
                return True
            else:
                logger.warning("éƒ¨åˆ†æŸ¥è¯¢ç»“æœå¯èƒ½æœªæˆåŠŸæäº¤ï¼Œä½†å°†ç»§ç»­å¤„ç†")
                return True

        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            logger.debug(traceback.format_exc())
            return False

    def submit_query_results_with_confirmation(self, task_id, query_results, max_retries=5):
        """
        æäº¤æŸ¥è¯¢ç»“æœå¹¶ç¡®ä¿æœåŠ¡å™¨æ¥æ”¶ï¼Œæ”¯æŒéƒ¨åˆ†å¤±è´¥é‡è¯•

        Args:
            task_id: ä»»åŠ¡ID
            query_results: æŸ¥è¯¢ç»“æœå­—å…¸
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            bool: æ˜¯å¦å…¨éƒ¨æˆåŠŸæäº¤
        """
        submit_url = urljoin(self.server_url, f"submit_query_results/{task_id}")

        pending_results = dict(query_results)
        all_successfully_received = []

        for attempt in range(max_retries):
            if not pending_results:
                logger.info("æ‰€æœ‰æŸ¥è¯¢ç»“æœå·²æˆåŠŸæäº¤")
                return True

            logger.info(f"å°è¯•æäº¤ {len(pending_results)} ä¸ªæŸ¥è¯¢ç»“æœï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰")

            batch_size = 20
            queries_to_submit = list(pending_results.keys())

            for i in range(0, len(queries_to_submit), batch_size):
                batch_queries = queries_to_submit[i:i + batch_size]
                batch_results = {q: pending_results[q] for q in batch_queries}

                try:
                    response = self._make_request('post', submit_url, json=batch_results, timeout=60)

                    if response and response.status_code == 200:
                        result = response.json()

                        successfully_received = result.get("successfully_received", [])
                        failed_queries = result.get("failed_queries", [])

                        logger.info(f"æ‰¹æ¬¡æäº¤ç»“æœ: æˆåŠŸ {len(successfully_received)}, å¤±è´¥ {len(failed_queries)}")

                        for query in successfully_received:
                            if query in pending_results:
                                del pending_results[query]
                                all_successfully_received.append(query)
                        if result.get("all_completed", False):
                            logger.info("æœåŠ¡å™¨ç¡®è®¤æ‰€æœ‰æŸ¥è¯¢å·²å®Œæˆï¼Œå¼€å§‹éªŒè¯æµç¨‹")
                            return True
                    else:
                        logger.error(f"æ‰¹æ¬¡æäº¤å¤±è´¥: HTTP {response.status_code if response else 'æ— å“åº”'}")

                except Exception as e:
                    logger.error(f"æäº¤æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")

            if pending_results and attempt < max_retries - 1:
                try:
                    check_url = urljoin(self.server_url, f"get_received_queries/{task_id}")
                    check_response = self._make_request('get', check_url, timeout=30)

                    if check_response and check_response.status_code == 200:
                        result = check_response.json()
                        received_queries = result.get("received_queries", [])

                        for query in list(pending_results.keys()):
                            if query in received_queries:
                                logger.info(f"æŸ¥è¯¢ '{query}' å·²è¢«æœåŠ¡å™¨æ¥æ”¶ï¼ˆé€šè¿‡ç¡®è®¤æ£€æŸ¥ï¼‰")
                                del pending_results[query]
                                if query not in all_successfully_received:
                                    all_successfully_received.append(query)

                except Exception as e:
                    logger.warning(f"æ£€æŸ¥å·²æ¥æ”¶æŸ¥è¯¢æ—¶å‡ºé”™: {e}")

                if pending_results:
                    wait_time = min(10 * (attempt + 1), 30)
                    logger.warning(f"è¿˜æœ‰ {len(pending_results)} ä¸ªæŸ¥è¯¢æœªæˆåŠŸæäº¤ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)

        if pending_results:
            logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»æœ‰ {len(pending_results)} ä¸ªæŸ¥è¯¢æœªèƒ½æäº¤")
            logger.error(f"æœªæäº¤çš„æŸ¥è¯¢: {list(pending_results.keys())[:5]}...")

            failed_results_path = f"failed_results_{task_id}_{int(time.time())}.json"
            try:
                with open(failed_results_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        "task_id": task_id,
                        "timestamp": time.time(),
                        "failed_queries": pending_results
                    }, f, ensure_ascii=False, indent=2)
                logger.info(f"æœªæäº¤çš„æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {failed_results_path}")
            except Exception as e:
                logger.error(f"ä¿å­˜å¤±è´¥ç»“æœæ—¶å‡ºé”™: {e}")

        return len(pending_results) == 0

    def check_task_status(self, task_id):
        """
        æ£€æŸ¥ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡çŠ¶æ€å­—å…¸æˆ–None(å¦‚æœå‡ºé”™)
        """
        try:
            url = urljoin(self.server_url, f"get_task_status/{task_id}")
            response = self._make_request('get', url, timeout=30)

            if response and response.status_code == 200:
                return response.json()
            else:
                status_code = response.status_code if response else "è¿æ¥å¤±è´¥"
                error_text = response.text if response else "æ— å“åº”"
                logger.error(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {status_code} - {error_text}")
                return None

        except Exception as e:
            logger.error(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return None

    def download_result(self, task_id, output_path):
        """
        ä¸‹è½½ä»»åŠ¡ç»“æœå¹¶ä¿å­˜åˆ°åˆé€‚çš„è·¯å¾„

        Args:
            task_id: ä»»åŠ¡ID
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            TrueæˆåŠŸï¼ŒFalseå¤±è´¥
        """
        try:
            url = urljoin(self.server_url, f"download_result/{task_id}")
            response = self._make_request('get', url, timeout=30)

            if response and response.status_code == 200:
                output_dir = os.path.dirname(output_path)
                if output_dir and not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)
                
                with open(output_path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"ç»“æœå·²ä¸‹è½½åˆ°: {output_path}")
                return True
            else:
                status_code = response.status_code if response else "è¿æ¥å¤±è´¥"
                error_text = response.text if response else "æ— å“åº”"
                logger.error(f"ä¸‹è½½ç»“æœå¤±è´¥: {status_code} - {error_text}")
                return False

        except Exception as e:
            logger.error(f"ä¸‹è½½ç»“æœæ—¶å‡ºé”™: {e}")
            return False

    def direct_verify(self, task_id):
        """
        ä½¿ç”¨Qwenç›´æ¥éªŒè¯(ä¸éœ€è¦å¤–éƒ¨æŸ¥è¯¢)

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            éªŒè¯ç»“æœæˆ–None(å¦‚æœå‡ºé”™)
        """
        try:
            url = urljoin(self.server_url, f"direct_verify/{task_id}")
            response = self._make_request('get', url, timeout=180)

            if response and response.status_code == 200:
                return response.json()
            else:
                status_code = response.status_code if response else "è¿æ¥å¤±è´¥"
                error_text = response.text if response else "æ— å“åº”"
                logger.error(f"ç›´æ¥éªŒè¯å¤±è´¥: {status_code} - {error_text}")
                return None

        except Exception as e:
            logger.error(f"ç›´æ¥éªŒè¯æ—¶å‡ºé”™: {e}")
            return None

    def run_complete_workflow(self, claim, media_path=None, output_path=None, direct_verify=False,
                              max_wait_time=600, query_wait_time=300):
        """
        æ‰§è¡Œå®Œæ•´çš„äº‹å®æ ¸æŸ¥å·¥ä½œæµç¨‹

        Args:
            claim: éœ€è¦æ ¸æŸ¥çš„å£°æ˜
            media_path: åª’ä½“æ–‡ä»¶è·¯å¾„(å¯é€‰)
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„(å¯é€‰)
            direct_verify: æ˜¯å¦ä½¿ç”¨Qwenç›´æ¥éªŒè¯
            max_wait_time: æœ€å¤§ç­‰å¾…éªŒè¯å®Œæˆæ—¶é—´(ç§’)ï¼Œé»˜è®¤10åˆ†é’Ÿ
            query_wait_time: ç­‰å¾…æŸ¥è¯¢ç”Ÿæˆçš„æœ€å¤§æ—¶é—´(ç§’)ï¼Œé»˜è®¤5åˆ†é’Ÿ

        Returns:
            ç»“æœå­—å…¸æˆ–None(å¦‚æœå‡ºé”™)
        """
        logger.info(f"å¼€å§‹å®Œæ•´å·¥ä½œæµç¨‹ï¼Œå£°æ˜: '{claim}'")

        if not self.check_server():
            logger.error("æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
            return None

        for attempt in range(3):
            if attempt > 0:
                logger.info(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æäº¤ä»»åŠ¡...")

            task_id = self.submit_task(claim, media_path)
            if task_id:
                break

            logger.warning(f"æäº¤ä»»åŠ¡å¤±è´¥ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
            time.sleep(5)

        if not task_id:
            logger.error("å¤šæ¬¡å°è¯•æäº¤ä»»åŠ¡å‡å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return None

        if direct_verify:
            logger.info("ä½¿ç”¨Qwenç›´æ¥éªŒè¯...")

            for attempt in range(3):
                if attempt > 0:
                    logger.info(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•ç›´æ¥éªŒè¯...")

                result = self.direct_verify(task_id)
                if result:
                    break

                logger.warning(f"ç›´æ¥éªŒè¯å¤±è´¥ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                time.sleep(5)

            if result and output_path:
                self._safe_save_result(result, output_path)

            return result

        logger.info("ç­‰å¾…æœåŠ¡å™¨å¤„ç†ä»»åŠ¡...")
        time.sleep(5)

        for attempt in range(3):
            if attempt > 0:
                logger.info(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤„ç†æŸ¥è¯¢...")

            if self.process_queries(task_id, max_wait_time=query_wait_time):
                break

            logger.warning(f"å¤„ç†æŸ¥è¯¢å¤±è´¥ï¼Œç­‰å¾…10ç§’åé‡è¯•...")
            time.sleep(10)

        start_time = time.time()
        completed = False
        result = None

        logger.info(f"ç­‰å¾…æœåŠ¡å™¨å®ŒæˆéªŒè¯ï¼Œæœ€é•¿ç­‰å¾… {max_wait_time} ç§’...")
        with tqdm(total=max_wait_time, desc="ç­‰å¾…éªŒè¯å®Œæˆ", unit="s") as pbar:
            while time.time() - start_time < max_wait_time:
                status = self.check_task_status(task_id)

                if status and status.get("status") == "completed":
                    completed = True
                    result = status.get("result")
                    break

                if status and status.get("status") == "error":
                    logger.error(f"ä»»åŠ¡å‡ºé”™: {status.get('error')}")
                    logger.debug(f"é”™è¯¯è¯¦æƒ…: {status.get('traceback')}")
                    break

                elapsed = min(int(time.time() - start_time), max_wait_time)
                pbar.update(elapsed - pbar.n)

                time.sleep(5)

        if not completed:
            logger.warning(f"ç­‰å¾…è¶…æ—¶ï¼Œä»»åŠ¡å¯èƒ½ä»åœ¨å¤„ç†ä¸­ã€‚ä»»åŠ¡ID: {task_id}")
            self.current_task_id = None
            return None

        if completed and output_path:
            if not self.download_result(task_id, output_path):
                if result:
                    self._safe_save_result(result, output_path)

        self.current_task_id = None
        return result

    def _safe_save_result(self, result, output_path):
        """
        å®‰å…¨åœ°ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼Œå¤„ç†è·¯å¾„é—®é¢˜
        
        Args:
            result: ç»“æœæ•°æ®
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            if output_path.startswith('/tmp/'):
                temp_dir = get_temp_dir()
                filename = os.path.basename(output_path)
                output_path = os.path.join(temp_dir, filename)
                logger.info(f"è¾“å‡ºè·¯å¾„å·²è°ƒæ•´ä¸º: {output_path}")
            
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
            try:
                fallback_path = f"result_{int(time.time())}.json"
                with open(fallback_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                logger.info(f"ç»“æœå·²ä¿å­˜åˆ°å¤‡ç”¨è·¯å¾„: {fallback_path}")
            except Exception as e2:
                logger.error(f"ä¿å­˜åˆ°å¤‡ç”¨è·¯å¾„ä¹Ÿå¤±è´¥: {e2}")


def main():
    parser = argparse.ArgumentParser(description="äº‹å®æ ¸æŸ¥ç³»ç»Ÿå®¢æˆ·ç«¯ (å¢å¼ºç‰ˆ)")
    parser.add_argument("--server", default="http://workspace.featurize.cn:35407", help="æœåŠ¡å™¨URL")
    parser.add_argument("--api-key", help="Google APIå¯†é’¥")
    parser.add_argument("--search-engine-id", help="Googleæœç´¢å¼•æ“ID")
    parser.add_argument("--claim", required=True, help="å¾…æ ¸æŸ¥çš„å£°æ˜")
    parser.add_argument("--media", help="åª’ä½“æ–‡ä»¶è·¯å¾„(mp4æˆ–jpg)")
    parser.add_argument("--output", default="result.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--direct", action="store_true", help="ä½¿ç”¨Qwenç›´æ¥éªŒè¯(ä¸æ‰§è¡Œç½‘ç»œæŸ¥è¯¢)")
    parser.add_argument("--max-wait", type=int, default=600, help="æœ€å¤§ç­‰å¾…éªŒè¯å®Œæˆæ—¶é—´(ç§’)")
    parser.add_argument("--query-wait", type=int, default=300, help="ç­‰å¾…æŸ¥è¯¢ç”Ÿæˆçš„æœ€å¤§æ—¶é—´(ç§’)")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ—¥å¿—")
    parser.add_argument("--max-retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--test-search", help="æµ‹è¯•æœç´¢æŸ¥è¯¢è€Œä¸æ‰§è¡Œå®Œæ•´æµç¨‹")

    args = parser.parse_args()

    if args.debug:
        logger.setLevel(logging.DEBUG)

    api_key = args.api_key or os.environ.get("GOOGLE_API_KEY")
    search_engine_id = args.search_engine_id or os.environ.get("GOOGLE_CSE_ID")

    if not api_key or not search_engine_id:
        logger.warning("æœªè®¾ç½®Google APIå¯†é’¥æˆ–æœç´¢å¼•æ“ID")

    client = FactCheckClient(args.server, api_key, search_engine_id, args.max_retries)

    try:
        if args.test_search:
            safe_print(f"\n===== æµ‹è¯•æœç´¢æŸ¥è¯¢: '{args.test_search}' =====")
            results = client.perform_search(args.test_search)
            safe_print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:")
            for i, result in enumerate(results, 1):
                safe_print(f"\n[{i}] {result}")
            return

        result = client.run_complete_workflow(
            args.claim,
            args.media,
            args.output,
            args.direct,
            args.max_wait,
            args.query_wait
        )

        if result:
            safe_print("\n===== äº‹å®æ ¸æŸ¥ç»“æœæ‘˜è¦ =====")

            if "direct_verification" in result:
                safe_print(f"ç›´æ¥éªŒè¯ç»“æœ: {result['direct_verification']}")
            else:
                judgment = result.get("final_judgment", {}).get("final_judgment", "uncertain")
                confidence = result.get("final_judgment", {}).get("confidence", 0)

                safe_print(f"å£°æ˜: {result.get('claim')}")
                safe_print(f"æœ€ç»ˆåˆ¤æ–­: {judgment}")
                safe_print(f"ç½®ä¿¡åº¦: {confidence}")

                evidence_count = len(result.get("evidence", []))
                safe_print(f"è¯æ®æ•°é‡: {evidence_count}")

            safe_print(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        else:
            safe_print("\n[ERROR] æ ¸æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯", use_emoji=False)

    except Exception as e:
        logger.critical(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}")
        logger.critical(traceback.format_exc())
        safe_print(f"\n[ERROR] ç¨‹åºå´©æºƒ: {str(e)}", use_emoji=False)
        safe_print("è¯·æŸ¥çœ‹æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()